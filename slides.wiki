== Introduction ==

==== Outline ====

\tableofcontents

=== Why Concurrency? ===

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

==== What is concurrency ====

* Parallel computing/processing

* Several computations executing simultaneously

* ... potentially interacting with each other

==== The brain is a parallel machine ====

* Asynchronous
* Distributed memory
* Simple messages
* Agnostic to component failure
* Connectivity:
** dense local
** sparse global
* Works with long latencies

* <2-2>... and works with messy data (big data)

=== Embarrassingly Parallel Programs ===

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

[frame]>

\begin{frame}{Useful Applications for Concurrency}{Ray Tracing}
\begin{figure}
\includegraphics[width=\textwidth]{images/raytracing}
\end{figure}
Trace the path from an imaginary eye (camera) through each pixel in a screen
and calculate the color of the object(s) visible through it.
\end{frame}

\begin{frame}{Useful Applications for Concurrency}{Ray Tracing}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{images/parallel_raytracing}
\caption{Ray Tracing performed by one task.}
\end{figure}
\column{0.5\textwidth}
\pause
\begin{figure}
\includegraphics[width=\textwidth]{images/parallel_raytracing2}
\caption{Ray Tracing performed by two tasks.}
\end{figure}
\end{columns}
\pause
\vfill
Ray Tracing is \textbf{embarrassingly parallel}:
\begin{itemize}
\item Little or no effort to separate the problem into parallel tasks
\item No dependencies or communication between the tasks
\end{itemize}
\end{frame}

=== Multicore Crisis ===

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

==== The free lunch is over ====

\begin{quote}
"Concurrency is the next major revolution in how we write 
software [after OOP]."
\end{quote}

--2cm--

Herb Sutter, ''The Free Lunch is Over: A Fundamental Turn Towards Concurrency in Software'' Dr.Dobb's Journal, 30(3) March 2005.

==== Multicore Crisis ====

* 1970-2005
** CPUs became quicker and quicker every year
** Moore’s Law: The number of transistors [...] doubles
** approximately every two years.

* _red_But!_
** Physical limits:
*** Miniaturization at atomic levels
*** energy consumption
*** heat produced by CPUs, etc.
** Stagnation in CPU clock rates since 2005

* Since 2005
** Chip producers aimed for more cores instead of higher clock rates.
** ==> Multicore crisis
** FLOPS/S has been raplaced by FLOPS/W
** (esp. in High Performance Computing(HPC))

==== Multicore Crisis ====

<[center]
    <<<images/clockspeeds.pdf, scale=0.4>>>
[center]>

==== Limitations: Amdahls Law====

\begin{quote}
The speedup of a program using multiple processors in parallel computing is
limited by the time needed for the sequential fraction of the program.
\end{quote}


==== Limitations: Amdahls Law====

<[center]
    <<<images/AmdahlsLaw.pdf, scale=0.3>>>
[center]>

\begin{quote}
\tiny{
if a program needs 20 hours using a single processor core, and a particular
portion of the program which takes one hour to execute cannot be parallelized,
while the remaining 19 hours ($95$) of execution time can be parallelized, then
regardless of how many processors are devoted to a parallelized execution of
this program, the minimum execution time cannot be less than that critical one
hour. Hence the speedup is limited up to 20×.}
\end{quote}


==== Don't Panic ====

* Writing parallel programs is easy!
* Small and simple APIs
* Designing parallel algorithms can be _green_easy_ or _red_hard_
* _green_Easy:_ embarassingly parallel
* _red_Hard:_ to find the parallelism

==== Don't Panic ====

* Scientific parallel programs are easy!
* _green_Parallelism:_ Number crunching over large datasets
* _green_Prior-art:_ Many algorithms already exist
* _green_Hardware:_ HPC is traditionally academic

== Symetric Multiprocessing (SMP) ==

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

[frame]>

<[nowiki]
\begin{frame}{Two Kinds of Tasks: Threads and Processes}
\begin{figure}
\includegraphics[width=0.5\textwidth]{images/processes_and_threads.pdf}
\end{figure}
\begin{itemize}
    \item A process has \textbf{one or more} threads
    \item Processes have their \textbf{own} memory (Variables, etc.)
    \item Threads share the memory of the process they belong to
    \item Threads are also called \textbf{lightweight} processes:
    \begin{itemize}
        \item They spawn faster than processes
        \item Context switches (if necessary) are faster
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Communication between Tasks}{Shared Memory and Message Passing}
Basically you have two paradigms:
\begin{enumerate}
    \item Shared Memory
    \begin{itemize}
        \item Taks A and B share some memory
        \item Whenever a task modifies a variable in the shared memory, the
        other task(s) see that change immediately
    \end{itemize}
    \item Message Passing
    \begin{itemize}
        \item Task A sends a message to Task B
        \item Task B receives the message and does something with it
    \end{itemize}
\end{enumerate}
The former paradigm is usually used with threads and the latter one with
processes (more on that later).
\end{frame}
[nowiki]>

==== Symmetric Multiprocessing (SMP) ====

* Homogeneous Multi-core and/or cpu
* Physically shared memory

* e.g. 8-way 6-core Opteron = 48 cores

* Most of you will have such a Laptop by now

==== CPython and the Global Interpreter Lock (GIL) ====

* CPython suffers from the ''Gil Problem''
* The interpreter has a global state storage
* Not thread-safe!

* Can have threads
* ... but only one can run at a time
* No real, concurrent threads in CPython

* <2-2> JYthon and IronPython do not suffer from this limitation


==== SMP in Python ====[containsverbatim]

* Standard as of Python 2.6

<[pyconcode]
>>> import multiprocessing
[pyconcode]>

* Like threads, but in separate processes
* Avoids GIL but higher process creation cost
* Package exists for 2.5


==== Race ====[containsverbatim]

* When unpredictable order of completion affects output
** Hardware latency
** Unpredictable algorithm run-time

* Difficult to debug because problematic case maybe infrequent

==== Race: the setup ====

\pyfile{code/ex_smp_race.py}

==== Race: in action ====[containsverbatim]

<[consolecode]
$ python ex_smp_race.py
10.0
$ python ex_smp_race.py
100.0
$ python ex_smp_race.py
10.0
[consolecode]>

==== The apparent solution ====[containsverbatim]

* Locks can be a solution to enforce atomicity:

<[pycode]
<[nowiki]
l = Lock()
l.acquire()
# <code>
l.release()
[nowiki]>
[pycode]>

* However, Locks are source of deadlocks


==== Deadlock: the setup ====[containsverbatim]

\pyfile{code/ex_smp.py}

==== Deadlock: in action ====[containsverbatim]

<[pyconcode]
<[nowiki]
>>> %run ex_smp.py
# p2 is still waiting for release (deadlock)
>>> p2.is_alive()
True
>>> arg.value()
10.0
# resolve the deadlock, without killing processes
>>> lock.release()
>>> p2.is_alive()
False
>>> p2.join()
>>> num.value
20
[nowiki]>
[pyconcode]>

See also: \href{http://en.wikipedia.org/wiki/Dining_philosophers_problem}{Dining Philosophers}

==== Shared Memory: Numpy and Multiprocessing ====[containsverbatim]

\pyfile{code/ex_smp_numpy.py}

* @cta@ is a synchronized shared memory buffer
* @npa@ is a numpy array that shares memory with @cta@

==== Shared Memory: Numpy and Multiprocessing ====[containsverbatim]

<[pyconcode]
>>> %run ex_smp_numpy.py
>>> npa
array([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,
         9.99997000e+05,   9.99998000e+05,   9.99999000e+05])
>>> p1.start(); p1.join()
In [6]: npa
array([  4.99999500e+11,   1.00000000e+00,   2.00000000e+00, ...,
         9.99997000e+05,   9.99998000e+05,   9.99999000e+05])
[pyconcode]>


==== Embarrassingly Parallel - Multiprocessing ====[containsverbatim]

* Imagine the following function that does work

\pyfile{code/ex_smp_emb.py}

==== The @Pool@ class and @map@ function ====[containsverbatim]

* Python provides a builtin @map@ function
* The @Pool@ class comes in handy for embarrassingly parallel problems
* Provides a @map@ function across worker processes

* @import@ statements need to be in the function
* If you need one with a progressbar: \href{https://github.com/esc/embparpbar}{embparpbar}

==== The @Pool@ class and @map@ function ====[containsverbatim]

<[pyconcode]

>>> nums = range(10)
>>> %timeit map(f, nums)
1 loops, best of 3: 1.37 s per loop
>>> pool1 = Pool(1)
>>> %timeit pool1.map(f, nums)
1 loops, best of 3: 1.27 s per loop
>>> pool6 = Pool(6)
>>> %timeit pool6.map(f, nums)
1 loops, best of 3: 281 ms per loop
>>> pool10 = Pool(10)
>>> %timeit pool10.map(f, nums)
10 loops, best of 3: 194 ms per loop
>>> pool16 = Pool(16)
>>> %timeit pool16.map(f, nums)
10 loops, best of 3: 186 ms per loop
[pyconcode]>

== Concurrency with IPython ==

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

==== Starting IPython engines ====[containsverbatim]

* To launch a small (or large) cluster of IPython engines

<[consolecode]
$ ipcluster start -n 24
2013-02-05 16:35:56,367.367 [IPClusterStart]
    Using existing profile dir: u'/home/val/.ipython/profile_default'
2013-02-05 16:35:56.372 [IPClusterStart]
    Starting ipcluster with [daemon=False]
2013-02-05 16:35:56.373 [IPClusterStart]
    Creating pid file: /home/val/.ipython/profile_default/pid/ipcluster.pid
2013-02-05 16:35:56.376 [IPClusterStart]
    Starting Controller with LocalControllerLauncher
2013-02-05 16:35:57.372 [IPClusterStart]
    Starting 24 Engines with LocalEngineSetLauncher
2013-02-05 16:36:29.888 [IPClusterStart]
    Engines appear to have started successfully
[consolecode]>

==== Command them from IPython ====[containsverbatim]

* Use the @Client@ class to interface with the engines

<[pyconcode]
>>> from IPython.parallel import Client
>>> client = Client()
>>> len(client.ids)
24
[pyconcode]>

==== Command them from IPython ====[containsverbatim]

* Easy example: using an embarrassingly parallel map
* Obtain a @DirectView@ using slicing on the @client@

<[pyconcode]
>>> dview = client[:]
>>> type(dview)
IPython.parallel.client.view.DirectView
>>> %timeit dview.map_sync(f, range(10)) # using the f from before
1 loops, best of 3: 187 ms per loop
[pyconcode]>

==== IPython engines have local namespaces ====[containsverbatim]

* @DirectView@ object provides dictionary access to engine namespaces

<[pyconcode]
>>> dview.execute('x=1')
>>> dview['x']
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
>>> dview8 = client[8]
>>> dview8.execute('x=23')
>>> dview['x']
[1, 1, 1, 1, 1, 1, 1, 1, 23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[pyconcode]>


==== IPython engines have local namespaces ====[containsverbatim]

* Using @apply@ in asynchronous and synchronous mode

<[pyconcode]
>>> result = dview.apply(lambda y: x+y, 19)
>>> type(result)
IPython.parallel.client.asyncresult.AsyncResult
>>> result.get()[7:12]
[20, 42, 20, 20, 20]
>>> dview.block = True
>>> dview.apply(lambda y: x+y, 19)
...
[pyconcode]>

==== Partition and Collection data ====[containsverbatim]

* Send using @scatter@

<[pyconcode]
>>> dview4 = client[:4]
>>> dview4.block = True
>>> dview4.scatter('a','Hello World!')
['Hel', 'lo ', 'Wor', 'ld!']
[pyconcode]>

* Now operate on the data using @execute@

<[pyconcode]
>>> dview4.execute('a = a.upper()', targets=[0, 1])
>>> dview4['a']
['HEL', 'LO ', 'Wor', 'ld!']
[pyconcode]>

* Collect using @gather@

<[pyconcode]
>>> "".join(dview4.gather('a'))
'HELLO World!'
[pyconcode]>

==== Warnings ====[containsverbatim]

* Engines are not reset after IPython restart

<[pyconcode]
>>> from IPython.parallel import Client
>>> client = Client()
>>> dview4 = client[:4]
>>> dview4['a']
['HEL', 'LO ', 'Wor', 'ld!']
[pyconcode]>

* Errors propagate as usual

<[nowiki]
\begin{minted}[fontsize=\footnotesize]{pycon}
>>> dview8['b']
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)<string> in <module>()
/home/val/anaconda/lib/python2.7/site-packages/IPython/parallel/util.py in _pull(keys)
    249     else:
    250         if not user_ns.has_key(keys):
--> 251             raise NameError("name '%s' is not defined"%keys)
    252         return user_ns.get(keys)
    253 
NameError: name 'b' is not defined
\end{minted}
[nowiki]>

==== Pros and Cons ====

<[block]{_green_Pros_}
* Interactive
* Plays nicely with MPI
* Re-conectible
* Debugging output from engines
* Can be run over other batch queueing systems
** (e.g. Sun Grid Engine)
[block]>

<[block]{_red_Cons_}
* Slow for large messages
* No shared memory
[block]>

== MPI for Python with @mpi4py@ ==

==== Outline ====

\tableofcontents[currentsection]

==== Scalable Message Passing Concurrency ====[containsverbatim]

* MPI - the Message Passing Interface
* Multi-process execution facilities

<[consolecode]
$ mpiexec -n 16 python helloworld.py
[consolecode]>

* API for inter-process message exchanges
** eg. Basic P2P: Send (emitter), Recv (consumer)
* Very common on large HPC installations

==== What is MPI for Python? (@mpi4py@) ====

* A wrapper for widely used MPI (MPICH2, OpenMPI, LAM/MPI)

* MPI supported by wide range of vendors, hardware, languages

* API based on the standard MPI-2 C++ bindings.

* Almost all MPI calls are supported.
** targeted to MPI-2 implementations.
** also works with MPI-1 implementations.

==== Basic stuff ====[containsverbatim]

<[pycode]
from mpi4py import MPI
comm = MPI.COMM_WORLD
[pycode]>


* Communicator = @comm@
** Manages processes and communication between them

* @MPI.COMM\_WORLD@
** all processes defined at execution time

* @Comm.size@ -- total number of available processes
* @Comm.rank@ -- ID of the current process

==== Basic stuff ====[containsverbatim]

\pyfile{code/ex_mpi.py}

<[consolecode]
$ mpiexec -n 4 python ex_mpi.py
Hello from teik, 1 of 4
Hello from teik, 3 of 4
Hello from teik, 2 of 4
Hello from teik, 0 of 4
[consolecode]>

* Process needs to check which @rank@ it is

==== What can this look like in practice ====

<[center]
    <<<images/bgp_mpi4py.png, width=\textwidth>>>
[center]>

==== Point-to-Point (P2P): Python objects ====

\pyfile{code/ex_mpi_p2p.py}

* The tag information allows selectivity of messages at the receiving end.

==== P2P: (NumPy) array data ====

<[nowiki]
\pyfile{code/ex_mpi_p2p_numpy.py}
[nowiki]>

* Array data buffer notation: @[<BUFFER>, MPI.<DATATYPE>]@
* Can also inferr the correct datatype from the numpy array
* MPI will write into the Numpy array

==== Collective Messages ====

Involve the whole @COMM@

* '''Scatter'''
** Spread a sequence over processes
* '''Gather'''
** Collect a sequence scattered over processes
* '''Broadcast'''
** Send a message to all processes
* '''Barrier'''
** Block till all processes arrive

==== Scatter and Gather ====

<[nowiki]
\inputminted[lastline=18, fontsize=\footnotesize, xleftmargin=12pt]{py}{code/ex_mpi_scatter_gather.py}
[nowiki]>

==== Scatter and Gather ====

<[nowiki]
\inputminted[firstline=19, fontsize=\footnotesize, xleftmargin=12pt]{py}{code/ex_mpi_scatter_gather.py}
[nowiki]>

==== What is happening here ====

* Imagine @comm.size == 2@
* @len(dest)@ is @5@ everywhere
* @msg@ is @array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])@
* @dest@ on @0@ becomes @array([0, 1, 2, 3, 4])@
* @dest@ on @1@ becomes @array([5, 6, 7, 8, 9])@

* <2-5>What is the @len@ of @ans@?
* <3-5> @2@ is correct
* <4-5>What is the final value of @ans@?
* <5-5> @[10, 35]@ is correct

==== The @import@ problem ====

* Importing Python module from each MPI process may cause problems on big installations
* \href{https://github.com/langton/MPI\_Import}{MPI\_import} provides specialised import statements that use MPI under the hood e.g. @mpi\_import@

<[nowiki]
\pyfile{code/ex_mpi_import.py}
[nowiki]>

==== Implementation ====

* Implemented with \href{http://www.cython.org}{Cython}

* Code base far easier to write, maintain, and extend.

* Faster than other solutions (mixed Python and C codes).

* A '''pythonic''' API that runs at C speed !


==== Portability ====

* Tested on all major platforms (Linux, Mac OS X, Windows).

* Works with the open-source MPI's (MPICH2, Open MPI, MPICH1, LAM).

* Should work with vendor-provided MPI's (HP, IBM, SGI).

* Works on Python 2.3 to 3.0 (Cython is just great!).


==== Interoperability ====

* Good support for wrapping other MPI-based codes.
* You can use Cython (@cimport@ statement).
* You can use boost.
* You can use SWIG (''typemaps'' provided).
* You can use F2Py (``py2f()``/``f2py()`` methods).
* You can use hand-written C (C-API provided).

* @mpi4py@ will allow you to use virtually '''any''' MPI based C/C++/Fortran
code from Python.

==== Features Summary ====

* Classical MPI-1 Point-to-Point.
** blocking (@send@/@recv@)
** non-blocking (@isend@/@irecv@, @test@/@wait@).
* Classical MPI-1 and Extended MPI-2 Collectives.
* Communication of general Python objects (pickle).
** very convenient, as general as pickle can be.
** can be slow for large data (CPU and memory consuming).
* Communication of array data (Python's buffer interface).
** MPI datatypes have to be explicitly specified.
** Very fast, almost C speed (for messages above 5-10 kB).
